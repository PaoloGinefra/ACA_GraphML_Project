{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf52ea62",
   "metadata": {},
   "source": [
    "# THIS IS A TEST FOR THE HGPSL THEN DISCARDED AS OUT OF SCOPE FOR THE PROJECT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a064170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ACAgraphML.Dataset import ZINC_Dataset\n",
    "from ACAgraphML.HGPSL import HGPSLModel\n",
    "from ACAgraphML.Transforms import OneHotEncodeFeat\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06d4809b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.loader import DataLoader\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from typing import Literal\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d6ae3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_PATH = './saved_models/HGPSL/'\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count()) if torch.cuda.is_available() else 0\n",
    "BATCH_SIZE = 256 if AVAIL_GPUS else 64\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Create Checkpoint path\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c51c6ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 10000\n",
      "Number of validation graphs: 1000\n",
      "Node feature shape: torch.Size([29, 28])\n",
      "Edge attribute shape after transform: torch.Size([64, 4])\n",
      "Target shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# Dataset configuration\n",
    "NUM_NODE_FEATS = 28  # Number of node features in ZINC dataset\n",
    "NUM_EDGE_FEATS = 4   # Number of edge features (bond types: 0, 1, 2, 3 where 0 is padding)\n",
    "oneHotTransform = OneHotEncodeFeat(NUM_NODE_FEATS)\n",
    "\n",
    "# Custom transform to ensure float type and process edge attributes\n",
    "def ensure_float_transform(data):\n",
    "    data = oneHotTransform(data)\n",
    "    data.x = data.x.float()  # Ensure node features are float\n",
    "    # Convert edge attributes to one-hot encoding for bond types\n",
    "    data.edge_attr = torch.nn.functional.one_hot(data.edge_attr.long(), num_classes=NUM_EDGE_FEATS).float()\n",
    "    return data\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = ZINC_Dataset.SMALL_TRAIN.load(transform=ensure_float_transform)\n",
    "val_dataset = ZINC_Dataset.SMALL_VAL.load(transform=ensure_float_transform)\n",
    "\n",
    "# Create data loaders\n",
    "graph_train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "graph_val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"Number of training graphs: {len(train_dataset)}\")\n",
    "print(f\"Number of validation graphs: {len(val_dataset)}\")\n",
    "print(f\"Node feature shape: {train_dataset[0].x.shape}\")\n",
    "print(f\"Edge attribute shape after transform: {train_dataset[0].edge_attr.shape}\")\n",
    "print(f\"Target shape: {train_dataset[0].y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4043dbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HGPSL Configuration:\n",
      "  num_features: 28\n",
      "  nhid: 128\n",
      "  num_classes: 1\n",
      "  pooling_ratio: 0.5\n",
      "  dropout_ratio: 0.5\n",
      "  sample_neighbor: True\n",
      "  sparse_attention: True\n",
      "  structure_learning: True\n",
      "  lamb: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Configuration class for HGPSL model (since it requires args object)\n",
    "class HGPSLConfig:\n",
    "    def __init__(self):\n",
    "        self.num_features = NUM_NODE_FEATS\n",
    "        self.nhid = 128  # Hidden dimension\n",
    "        self.num_classes = 1  # Regression task\n",
    "        self.pooling_ratio = 0.5  # Pooling ratio for hierarchical pooling\n",
    "        self.dropout_ratio = 0.5  # Dropout ratio\n",
    "        self.sample_neighbor = True  # Whether to sample neighbors\n",
    "        self.sparse_attention = True  # Whether to use sparse attention\n",
    "        self.structure_learning = True  # Whether to use structure learning\n",
    "        self.lamb = 1.0  # Lambda parameter for structure learning\n",
    "\n",
    "# Create configuration\n",
    "hgpsl_config = HGPSLConfig()\n",
    "print(\"HGPSL Configuration:\")\n",
    "for attr, value in vars(hgpsl_config).items():\n",
    "    print(f\"  {attr}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a313581",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGPSLLightningModule(pl.LightningModule):\n",
    "    def __init__(self, args, loss: Literal['mse', 'mae'] = 'mae', lr: float = 1e-3, weight_decay: float = 0.0):\n",
    "        super().__init__()\n",
    "        # Saving hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Initialize HGPSL model\n",
    "        self.model = HGPSLModel(args)\n",
    "        \n",
    "        # Loss function\n",
    "        if loss == 'mae':\n",
    "            self.loss_module = nn.L1Loss()\n",
    "        elif loss == 'mse':\n",
    "            self.loss_module = nn.MSELoss()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported loss type: {loss}\")\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, data, mode=\"train\"):\n",
    "        # HGPSL expects log_softmax output, but we need regression output\n",
    "        # We'll need to modify the forward pass slightly\n",
    "        x = self.model(data)\n",
    "        \n",
    "        # Since HGPSL outputs log_softmax for classification, \n",
    "        # we need to modify it for regression\n",
    "        # For now, let's just remove the log_softmax and treat it as regression\n",
    "        x = x.squeeze(dim=-1) if x.dim() > 1 else x\n",
    "        \n",
    "        if hasattr(data, 'y'):\n",
    "            loss = self.loss_module(x, data.y)\n",
    "            return x, loss\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.forward(batch, mode=\"train\")[1]\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y, loss = self.forward(batch, mode=\"val\")\n",
    "        self.log(\"val_loss\", loss)\n",
    "        mae = nn.L1Loss()(y, batch.y)\n",
    "        self.log(\"val_mae\", mae)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        y, loss = self.forward(batch, mode=\"test\")\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        mae = nn.L1Loss()(y, batch.y)   \n",
    "        self.log(\"test_mae\", mae, prog_bar=True)\n",
    "        return {\"test_loss\": loss, \"test_mae\": mae}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1724356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ACAgraphML.HGPSL.layers import GCN, HGPSLPool\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool as gap, global_max_pool as gmp\n",
    "\n",
    "class HGPSLRegressionModel(torch.nn.Module):\n",
    "    \"\"\"Modified HGPSL model for regression tasks\"\"\"\n",
    "    def __init__(self, args):\n",
    "        super(HGPSLRegressionModel, self).__init__()\n",
    "        self.args = args\n",
    "        self.num_features = args.num_features\n",
    "        self.nhid = args.nhid\n",
    "        self.num_classes = args.num_classes\n",
    "        self.pooling_ratio = args.pooling_ratio\n",
    "        self.dropout_ratio = args.dropout_ratio\n",
    "        self.sample = args.sample_neighbor\n",
    "        self.sparse = args.sparse_attention\n",
    "        self.sl = args.structure_learning\n",
    "        self.lamb = args.lamb\n",
    "\n",
    "        self.conv1 = GCNConv(self.num_features, self.nhid)\n",
    "        self.conv2 = GCN(self.nhid, self.nhid)\n",
    "        self.conv3 = GCN(self.nhid, self.nhid)\n",
    "\n",
    "        self.pool1 = HGPSLPool(self.nhid, self.pooling_ratio,\n",
    "                               self.sample, self.sparse, self.sl, self.lamb)\n",
    "        self.pool2 = HGPSLPool(self.nhid, self.pooling_ratio,\n",
    "                               self.sample, self.sparse, self.sl, self.lamb)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(self.nhid * 2, self.nhid)\n",
    "        self.lin2 = torch.nn.Linear(self.nhid, self.nhid // 2)\n",
    "        self.lin3 = torch.nn.Linear(self.nhid // 2, self.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        edge_attr = None\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        x, edge_index, edge_attr, batch = self.pool1(\n",
    "            x, edge_index, edge_attr, batch)\n",
    "        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
    "        x, edge_index, edge_attr, batch = self.pool2(\n",
    "            x, edge_index, edge_attr, batch)\n",
    "        x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv3(x, edge_index, edge_attr))\n",
    "        x3 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(x1) + F.relu(x2) + F.relu(x3)\n",
    "\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=self.dropout_ratio, training=self.training)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.dropout(x, p=self.dropout_ratio, training=self.training)\n",
    "        # For regression, we don't need log_softmax, just linear output\n",
    "        x = self.lin3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21ece41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGPSLRegressionLightningModule(pl.LightningModule):\n",
    "    def __init__(self, args, loss: Literal['mse', 'mae'] = 'mae', lr: float = 1e-3, weight_decay: float = 0.0):\n",
    "        super().__init__()\n",
    "        # Saving hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Initialize HGPSL regression model\n",
    "        self.model = HGPSLRegressionModel(args)\n",
    "        \n",
    "        # Loss function\n",
    "        if loss == 'mae':\n",
    "            self.loss_module = nn.L1Loss()\n",
    "        elif loss == 'mse':\n",
    "            self.loss_module = nn.MSELoss()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported loss type: {loss}\")\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, data, mode=\"train\"):\n",
    "        x = self.model(data)\n",
    "        x = x.squeeze(dim=-1) if x.dim() > 1 else x\n",
    "        \n",
    "        if hasattr(data, 'y'):\n",
    "            loss = self.loss_module(x, data.y)\n",
    "            return x, loss\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.forward(batch, mode=\"train\")[1]\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y, loss = self.forward(batch, mode=\"val\")\n",
    "        self.log(\"val_loss\", loss)\n",
    "        mae = nn.L1Loss()(y, batch.y)\n",
    "        self.log(\"val_mae\", mae)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        y, loss = self.forward(batch, mode=\"test\")\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        mae = nn.L1Loss()(y, batch.y)   \n",
    "        self.log(\"test_mae\", mae, prog_bar=True)\n",
    "        return {\"test_loss\": loss, \"test_mae\": mae}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5df0103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hgpsl_model(config, max_epochs=100, loss='mae', lr=1e-3, weight_decay=0.0):\n",
    "    \"\"\"\n",
    "    Train HGPSL model with PyTorch Lightning\n",
    "    \n",
    "    Args:\n",
    "        config: HGPSLConfig object with model parameters\n",
    "        max_epochs: Maximum number of epochs to train\n",
    "        loss: Loss function ('mae' or 'mse')\n",
    "        lr: Learning rate\n",
    "        weight_decay: Weight decay for optimizer\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained model\n",
    "        result: Dictionary with training results\n",
    "    \"\"\"\n",
    "    pl.seed_everything(42)\n",
    "\n",
    "    # Create a PyTorch Lightning trainer\n",
    "    root_dir = os.path.join(CHECKPOINT_PATH,\n",
    "                            f\"HGPSL_nhid_{config.nhid}\" +\n",
    "                            f\"_loss_{loss}\" +\n",
    "                            f\"_pooling_{config.pooling_ratio}\" +\n",
    "                            f\"_dropout_{config.dropout_ratio}\" +\n",
    "                            f\"_lr_{lr}\" +\n",
    "                            f\"_wd_{weight_decay}\")\n",
    "    \n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=root_dir,\n",
    "        callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"min\", monitor=\"val_loss\")],\n",
    "        accelerator=\"cpu\" if AVAIL_GPUS == 0 else \"gpu\",\n",
    "        devices=max(1, AVAIL_GPUS),\n",
    "        max_epochs=max_epochs,\n",
    "        enable_progress_bar=True,\n",
    "    )\n",
    "    trainer.logger._default_hp_metric = None\n",
    "\n",
    "    pl.seed_everything(42)\n",
    "    model = HGPSLRegressionLightningModule(\n",
    "        args=config,\n",
    "        loss=loss,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.fit(model, graph_train_loader, graph_val_loader)\n",
    "    \n",
    "    # Load best model\n",
    "    model = HGPSLRegressionLightningModule.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    # Test best model on training and validation sets\n",
    "    train_result = trainer.test(model, dataloaders=graph_train_loader, verbose=False)\n",
    "    val_result = trainer.test(model, dataloaders=graph_val_loader, verbose=False)\n",
    "    \n",
    "    result = {\n",
    "        \"train\": train_result[0][\"test_loss\"], \n",
    "        \"val\": val_result[0][\"test_loss\"],\n",
    "        \"train_mae\": train_result[0][\"test_mae\"],\n",
    "        \"val_mae\": val_result[0][\"test_mae\"]\n",
    "    }\n",
    "    \n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ab2cc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "\n",
      "  | Name        | Type                 | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model       | HGPSLRegressionModel | 5.8 K  | train\n",
      "1 | loss_module | L1Loss               | 0      | train\n",
      "-------------------------------------------------------------\n",
      "5.8 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.8 K     Total params\n",
      "0.023     Total estimated model params size (MB)\n",
      "20        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "\n",
      "  | Name        | Type                 | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model       | HGPSLRegressionModel | 5.8 K  | train\n",
      "1 | loss_module | L1Loss               | 0      | train\n",
      "-------------------------------------------------------------\n",
      "5.8 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.8 K     Total params\n",
      "0.023     Total estimated model params size (MB)\n",
      "20        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing HGPSL model with minimal configuration...\n",
      "Test Configuration:\n",
      "  num_features: 28\n",
      "  nhid: 32\n",
      "  num_classes: 1\n",
      "  pooling_ratio: 0.8\n",
      "  dropout_ratio: 0.2\n",
      "  sample_neighbor: True\n",
      "  sparse_attention: True\n",
      "  structure_learning: False\n",
      "  lamb: 1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf27138748a46298832973c139b45ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf993ec085b41a0a0fd06998c4e5542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61941a1110a5407795cf417e9c0a61ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee50f489d7f448c8dda4b89799ff0be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb8ffde44314c359c985c11d0a2459a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "271999b5e07842c69c759df3c5c5699f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1eb1f122f4d43c7ad3a7bd1fddbac4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:476: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:476: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a036a0e0ac4a54ae6f603c3dfbf7ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1516. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1463. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1427. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1427. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1508. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1508. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1439. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1450. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1514. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1439. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1450. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1514. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1483. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1442. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1529. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1483. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1442. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1529. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1521. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1524. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1478. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1521. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1524. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1478. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1512. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1498. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1512. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1498. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1568. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1568. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1505. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1525. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1486. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1523. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1505. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1525. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1486. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1523. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1535. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1554. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1535. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1554. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1520. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1476. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1488. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1507. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1497. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1520. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1476. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1488. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1507. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1497. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1515. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1515. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1504. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1425. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1503. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1504. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1425. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1503. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1473. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1479. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1473. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1479. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1477. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1495. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1487. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1477. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1495. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1487. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1558. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1558. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1519. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1517. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1519. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1517. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1492. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1448. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1467. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1455. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1502. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1492. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1448. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1467. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1455. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1502. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1459. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1446. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1550. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1360. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1459. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1446. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1550. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1360. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1412. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1457. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1412. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1457. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1468. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1471. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1456. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1475. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1511. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1468. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1471. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1456. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1475. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1511. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1484. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1484. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1493. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1491. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1513. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1432. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1493. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1491. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1513. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1432. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1531. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1451. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1531. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1451. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1470. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1470. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1418. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1431. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1418. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1431. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1537. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1430. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1537. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1430. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1489. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1544. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1489. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1544. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1429. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1465. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1429. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1465. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1533. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 378. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1533. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 378. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e9203894a2240c38357083075b1cb71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quick Test Results:\n",
      "Train Loss: 0.8067, Train MAE: 0.8067\n",
      "Val Loss: 0.7718, Val MAE: 0.7718\n"
     ]
    }
   ],
   "source": [
    "# Test the model with basic configuration - Quick test first\n",
    "print(\"Testing HGPSL model with minimal configuration...\")\n",
    "\n",
    "# Create a smaller config for testing\n",
    "test_config = HGPSLConfig()\n",
    "test_config.nhid = 32  # Smaller hidden dimension\n",
    "test_config.pooling_ratio = 0.8  # Keep more nodes\n",
    "test_config.dropout_ratio = 0.2  # Less dropout\n",
    "test_config.structure_learning = False  # Disable structure learning for faster testing\n",
    "\n",
    "print(\"Test Configuration:\")\n",
    "for attr, value in vars(test_config).items():\n",
    "    print(f\"  {attr}: {value}\")\n",
    "\n",
    "try:\n",
    "    model, result = train_hgpsl_model(\n",
    "        config=test_config,\n",
    "        max_epochs=5,  # Very few epochs for quick test\n",
    "        loss='mae',\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "\n",
    "    print(\"\\nQuick Test Results:\")\n",
    "    print(f\"Train Loss: {result['train']:.4f}, Train MAE: {result['train_mae']:.4f}\")\n",
    "    print(f\"Val Loss: {result['val']:.4f}, Val MAE: {result['val_mae']:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during quick test: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4c84533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "\n",
      "  | Name        | Type                 | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model       | HGPSLRegressionModel | 20.8 K | train\n",
      "1 | loss_module | L1Loss               | 0      | train\n",
      "-------------------------------------------------------------\n",
      "20.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "20.8 K    Total params\n",
      "0.083     Total estimated model params size (MB)\n",
      "20        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\n",
      "  | Name        | Type                 | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model       | HGPSLRegressionModel | 20.8 K | train\n",
      "1 | loss_module | L1Loss               | 0      | train\n",
      "-------------------------------------------------------------\n",
      "20.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "20.8 K    Total params\n",
      "0.083     Total estimated model params size (MB)\n",
      "20        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING WITH OPTIMIZED CONFIGURATION\n",
      "============================================================\n",
      "Optimized Configuration:\n",
      "  num_features: 28\n",
      "  nhid: 64\n",
      "  num_classes: 1\n",
      "  pooling_ratio: 0.6\n",
      "  dropout_ratio: 0.3\n",
      "  sample_neighbor: True\n",
      "  sparse_attention: True\n",
      "  structure_learning: True\n",
      "  lamb: 1.0\n",
      "\n",
      "Starting optimized training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f1428cc3ec42faa5bc02068e34f116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1404670f64f440669869832c77b319bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4499045566394a8cb6f7bb9cf87d42de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf6f691f11084c80aac5ef87d79fec0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a6f784ab55045fe8441aced8e74f043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce7bf9371694ce1a9e39f5a6efb5daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee9aedceb4a5498e8d3f5b9e5a4f69d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1865499e0aa04a14a558165bb7a7f307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c3bacc01b6c475c9c5368babc68843e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb07d26198b47ef81f2051a42f70466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d13dde3cd54efd840811b8dd80b6f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52ab100d7204cd289fe229dc4526b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b465d6f96404450be1f3b1a4ce4e824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ffef1cdaf74c3fa0aa64d45a3e7f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2f592f1f8542dd90adc06cff84a7a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2550f8c7b9204a108c30f86fdf23cf30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f35f0d542a4202b7483e3d3a84e8bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f3023cc79047baa2292ed147b3b225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "881d4725e463468eb6cf31dbfd10cacf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1533447c1656409d97499322454b66e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc794a855a448418237777541c9f113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0871339f90644edaa7ac6cffbd254716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf6b2a2e85a4e5586b1265c24d1a7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40dda4f0d959401ca16cc1289bc165db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8913541df28c4d0f8faa35fac8f402ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf3b6a3eb2f4892b16150ff46b11068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "782da10cd29e45f4be7d2a805786378b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4685da0236a74c99aa70cc34e71dab73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1413. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1403. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1424. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1424. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1506. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1490. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1506. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1490. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1499. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1499. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1569. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1569. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1458. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1458. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1522. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1541. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1522. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1541. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1437. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1437. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1441. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1441. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1462. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1462. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1460. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1460. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1454. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1449. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1454. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1449. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1419. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1419. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1540. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1540. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1440. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1440. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1538. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1538. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1560. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1560. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1469. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1469. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1528. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1528. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1496. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1496. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1405. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1405. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1482. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1482. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1433. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1433. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1426. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1426. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1539. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 392. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1539. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Polimi\\Master\\2Sem\\ACA_GraphML_Project\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 392. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12b5909976742a599db3c5321989348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimized Training Results:\n",
      "Train Loss: 0.7217, Train MAE: 0.7217\n",
      "Val Loss: 0.7040, Val MAE: 0.7040\n"
     ]
    }
   ],
   "source": [
    "# Now let's train with a more reasonable configuration\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING WITH OPTIMIZED CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create optimized config for better performance\n",
    "optimized_config = HGPSLConfig()\n",
    "optimized_config.nhid = 64  # Reasonable hidden dimension\n",
    "optimized_config.pooling_ratio = 0.6  # Moderate pooling\n",
    "optimized_config.dropout_ratio = 0.3  # Moderate dropout\n",
    "optimized_config.structure_learning = True  # Enable structure learning\n",
    "optimized_config.sparse_attention = True  # Keep sparse attention\n",
    "optimized_config.sample_neighbor = True  # Sample neighbors for efficiency\n",
    "\n",
    "print(\"Optimized Configuration:\")\n",
    "for attr, value in vars(optimized_config).items():\n",
    "    print(f\"  {attr}: {value}\")\n",
    "\n",
    "print(\"\\nStarting optimized training...\")\n",
    "optimized_model, optimized_result = train_hgpsl_model(\n",
    "    config=optimized_config,\n",
    "    max_epochs=25,  # Reasonable number of epochs\n",
    "    loss='mae',\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "print(\"\\nOptimized Training Results:\")\n",
    "print(f\"Train Loss: {optimized_result['train']:.4f}, Train MAE: {optimized_result['train_mae']:.4f}\")\n",
    "print(f\"Val Loss: {optimized_result['val']:.4f}, Val MAE: {optimized_result['val_mae']:.4f}\")\n",
    "\n",
    "# Store the best model for later use\n",
    "best_model = optimized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b318ea6",
   "metadata": {},
   "source": [
    "## HGPSL Model Overview\n",
    "\n",
    "The **Hierarchical Graph Pooling with Structure Learning (HGPSL)** model is an advanced graph neural network that combines:\n",
    "\n",
    "### Key Features:\n",
    "1. **Hierarchical Pooling**: Progressively reduces graph size while preserving important information\n",
    "2. **Structure Learning**: Can learn new graph connections beyond the original structure\n",
    "3. **Information Score**: Uses node information scores to determine which nodes to keep during pooling\n",
    "4. **Multi-level Representations**: Combines features from different levels of the hierarchy\n",
    "\n",
    "### Performance Characteristics:\n",
    "- **Computational Cost**: Higher than basic GNNs due to structure learning and hierarchical processing\n",
    "- **Memory Usage**: Moderate, with efficient sparse operations\n",
    "- **Best For**: Graph-level tasks where hierarchical structure matters (molecular property prediction, etc.)\n",
    "\n",
    "### Configuration Tips:\n",
    "- **`nhid`**: Hidden dimension (32-128 for ZINC dataset)\n",
    "- **`pooling_ratio`**: Fraction of nodes to keep (0.5-0.8 recommended)\n",
    "- **`structure_learning`**: Enable for better performance, disable for speed\n",
    "- **`sample_neighbor`**: Keep enabled for large graphs\n",
    "- **`sparse_attention`**: Keep enabled for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6161f2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_predictions(model, data_loader):\n",
    "    \"\"\"Compute predictions for a given data loader\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for batch in data_loader:\n",
    "        preds = model(batch)\n",
    "        predictions.append(preds)\n",
    "    return torch.cat(predictions, dim=0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_embeddings(model, data_loader):\n",
    "    \"\"\"Compute embeddings before the final linear layers\"\"\"\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    for batch in data_loader:\n",
    "        # Get intermediate representations before final linear layers\n",
    "        x, edge_index, batch_idx = batch.x, batch.edge_index, batch.batch\n",
    "        edge_attr = None\n",
    "\n",
    "        # Forward through GNN layers\n",
    "        x = F.relu(model.model.conv1(x, edge_index, edge_attr))\n",
    "        x, edge_index, edge_attr, batch_idx = model.model.pool1(x, edge_index, edge_attr, batch_idx)\n",
    "        x1 = torch.cat([gmp(x, batch_idx), gap(x, batch_idx)], dim=1)\n",
    "\n",
    "        x = F.relu(model.model.conv2(x, edge_index, edge_attr))\n",
    "        x, edge_index, edge_attr, batch_idx = model.model.pool2(x, edge_index, edge_attr, batch_idx)\n",
    "        x2 = torch.cat([gmp(x, batch_idx), gap(x, batch_idx)], dim=1)\n",
    "\n",
    "        x = F.relu(model.model.conv3(x, edge_index, edge_attr))\n",
    "        x3 = torch.cat([gmp(x, batch_idx), gap(x, batch_idx)], dim=1)\n",
    "\n",
    "        x = F.relu(x1) + F.relu(x2) + F.relu(x3)\n",
    "        embeddings.append(x)\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "def plot_predictions(model, train_loader, val_loader, train_dataset, val_dataset):\n",
    "    \"\"\"Plot predictions vs true values\"\"\"\n",
    "    train_preds = compute_predictions(model, train_loader)\n",
    "    val_preds = compute_predictions(model, val_loader)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Training predictions\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(train_dataset.y.cpu().numpy(), train_preds.cpu().numpy(), alpha=0.5, s=2)\n",
    "    plt.plot([train_dataset.y.min(), train_dataset.y.max()], \n",
    "             [train_dataset.y.min(), train_dataset.y.max()], 'r--', lw=2)\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.title('Training Set')\n",
    "    \n",
    "    # Validation predictions\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(val_dataset.y.cpu().numpy(), val_preds.cpu().numpy(), alpha=0.5, s=2)\n",
    "    plt.plot([val_dataset.y.min(), val_dataset.y.max()], \n",
    "             [val_dataset.y.min(), val_dataset.y.max()], 'r--', lw=2)\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.title('Validation Set')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Use the best model if available, otherwise use the test model\n",
    "model_to_plot = best_model if 'best_model' in locals() else model\n",
    "print(f\"Using model for plotting: {type(model_to_plot).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03e6663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and visualize the best model\n",
    "print(\"Plotting predictions vs true values...\")\n",
    "model_to_use = best_model if 'best_model' in locals() else model\n",
    "plot_predictions(model_to_use, graph_train_loader, graph_val_loader, train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b4a410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter experimentation\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"HYPERPARAMETER EXPERIMENTATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "results_comparison = []\n",
    "\n",
    "# Test different hidden dimensions\n",
    "for nhid in [64, 128, 256]:\n",
    "    config = HGPSLConfig()\n",
    "    config.nhid = nhid\n",
    "    print(f\"\\nTesting with hidden dimension: {nhid}\")\n",
    "    \n",
    "    model_exp, result_exp = train_hgpsl_model(\n",
    "        config=config,\n",
    "        max_epochs=30,  # Shorter for experimentation\n",
    "        loss='mae',\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "    \n",
    "    results_comparison.append({\n",
    "        'nhid': nhid,\n",
    "        'val_mae': result_exp['val_mae']\n",
    "    })\n",
    "    \n",
    "    print(f\"Val MAE: {result_exp['val_mae']:.4f}\")\n",
    "\n",
    "# Test different pooling ratios\n",
    "for pooling_ratio in [0.3, 0.5, 0.7]:\n",
    "    config = HGPSLConfig()\n",
    "    config.pooling_ratio = pooling_ratio\n",
    "    print(f\"\\nTesting with pooling ratio: {pooling_ratio}\")\n",
    "    \n",
    "    model_exp, result_exp = train_hgpsl_model(\n",
    "        config=config,\n",
    "        max_epochs=30,\n",
    "        loss='mae',\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "    \n",
    "    results_comparison.append({\n",
    "        'pooling_ratio': pooling_ratio,\n",
    "        'val_mae': result_exp['val_mae']\n",
    "    })\n",
    "    \n",
    "    print(f\"Val MAE: {result_exp['val_mae']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPERIMENTAL RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "for result in results_comparison:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee522af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embeddings\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EMBEDDING VISUALIZATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Compute embeddings\n",
    "print(\"Computing embeddings...\")\n",
    "train_embeddings = compute_embeddings(model, graph_train_loader)\n",
    "val_embeddings = compute_embeddings(model, graph_val_loader)\n",
    "\n",
    "print(f\"Training embeddings shape: {train_embeddings.shape}\")\n",
    "print(f\"Validation embeddings shape: {val_embeddings.shape}\")\n",
    "\n",
    "# t-SNE visualization\n",
    "try:\n",
    "    from sklearn.manifold import TSNE\n",
    "    print(\"Performing t-SNE visualization...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42, verbose=1)\n",
    "    val_embeddings_2d = tsne.fit_transform(val_embeddings.cpu().numpy())\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(val_embeddings_2d[:, 0], val_embeddings_2d[:, 1], \n",
    "                c=val_dataset.y.cpu().numpy(), cmap='viridis', s=10)\n",
    "    plt.colorbar(label='Target Value')\n",
    "    plt.title('t-SNE Visualization of HGPSL Validation Embeddings')\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.show()\n",
    "except ImportError:\n",
    "    print(\"scikit-learn not available for t-SNE visualization\")\n",
    "\n",
    "# UMAP visualization\n",
    "try:\n",
    "    import umap\n",
    "    print(\"Performing UMAP visualization...\")\n",
    "    umap_model = umap.UMAP(n_components=2, random_state=42, verbose=True)\n",
    "    val_embeddings_2d_umap = umap_model.fit_transform(val_embeddings.cpu().numpy())\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(val_embeddings_2d_umap[:, 0], val_embeddings_2d_umap[:, 1], \n",
    "                c=val_dataset.y.cpu().numpy(), cmap='viridis', s=10)\n",
    "    plt.colorbar(label='Target Value')\n",
    "    plt.title('UMAP Visualization of HGPSL Validation Embeddings')\n",
    "    plt.xlabel('UMAP Component 1')\n",
    "    plt.ylabel('UMAP Component 2')\n",
    "    plt.show()\n",
    "except ImportError:\n",
    "    print(\"UMAP not available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f383569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with traditional ML models using HGPSL embeddings\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPARISON WITH TRADITIONAL ML MODELS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# XGBoost on HGPSL embeddings\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    \n",
    "    print(\"Training XGBoost on HGPSL embeddings...\")\n",
    "    xgb_model = XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_estimators=1000,\n",
    "        max_depth=10,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    xgb_model.fit(\n",
    "        train_embeddings.cpu().numpy(),\n",
    "        train_dataset.y.cpu().numpy(),\n",
    "        eval_set=[(val_embeddings.cpu().numpy(), val_dataset.y.cpu().numpy())],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    xgb_val_predictions = xgb_model.predict(val_embeddings.cpu().numpy())\n",
    "    xgb_train_predictions = xgb_model.predict(train_embeddings.cpu().numpy())\n",
    "    \n",
    "    # Calculate MAE\n",
    "    xgb_val_mae = mean_absolute_error(val_dataset.y.cpu().numpy(), xgb_val_predictions)\n",
    "    xgb_train_mae = mean_absolute_error(train_dataset.y.cpu().numpy(), xgb_train_predictions)\n",
    "    \n",
    "    print(f\"XGBoost - Train MAE: {xgb_train_mae:.4f}, Val MAE: {xgb_val_mae:.4f}\")\n",
    "    \n",
    "    # Plot XGBoost predictions\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(train_dataset.y.cpu().numpy(), xgb_train_predictions, alpha=0.5, s=2, color='blue')\n",
    "    plt.plot([train_dataset.y.min(), train_dataset.y.max()], \n",
    "             [train_dataset.y.min(), train_dataset.y.max()], 'r--', lw=2)\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('XGBoost Predictions')\n",
    "    plt.title('XGBoost on HGPSL Embeddings - Training')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(val_dataset.y.cpu().numpy(), xgb_val_predictions, alpha=0.5, s=2, color='orange')\n",
    "    plt.plot([val_dataset.y.min(), val_dataset.y.max()], \n",
    "             [val_dataset.y.min(), val_dataset.y.max()], 'r--', lw=2)\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('XGBoost Predictions')\n",
    "    plt.title('XGBoost on HGPSL Embeddings - Validation')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"XGBoost not available\")\n",
    "\n",
    "# Random Forest on HGPSL embeddings\n",
    "try:\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    print(\"Training Random Forest on HGPSL embeddings...\")\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=1000,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    rf_model.fit(\n",
    "        train_embeddings.cpu().numpy(),\n",
    "        train_dataset.y.cpu().numpy()\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    rf_val_predictions = rf_model.predict(val_embeddings.cpu().numpy())\n",
    "    rf_train_predictions = rf_model.predict(train_embeddings.cpu().numpy())\n",
    "    \n",
    "    # Calculate MAE\n",
    "    rf_val_mae = mean_absolute_error(val_dataset.y.cpu().numpy(), rf_val_predictions)\n",
    "    rf_train_mae = mean_absolute_error(train_dataset.y.cpu().numpy(), rf_train_predictions)\n",
    "    \n",
    "    print(f\"Random Forest - Train MAE: {rf_train_mae:.4f}, Val MAE: {rf_val_mae:.4f}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"scikit-learn not available for Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc2b856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and Conclusions\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SUMMARY AND CONCLUSIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\"\"\n",
    "## HGPSL Model Testing Summary\n",
    "\n",
    "This notebook demonstrates a complete PyTorch Lightning pipeline for testing the HGPSL \n",
    "(Hierarchical Graph Pooling with Structure Learning) model on the ZINC molecular dataset.\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **PyTorch Lightning Module**: HGPSLRegressionLightningModule\n",
    "   - Wraps the HGPSL model for regression tasks\n",
    "   - Handles training and validation loops\n",
    "   - Supports MAE and MSE loss functions\n",
    "   - Includes proper optimizer configuration\n",
    "\n",
    "2. **Modified HGPSL Model**: HGPSLRegressionModel\n",
    "   - Adapted from the original classification model\n",
    "   - Removed log_softmax for regression output\n",
    "   - Maintains hierarchical pooling capabilities\n",
    "\n",
    "3. **Training Pipeline**: train_hgpsl_model()\n",
    "   - Automated training with checkpointing\n",
    "   - Model selection based on validation loss\n",
    "   - Comprehensive result reporting\n",
    "\n",
    "4. **Evaluation and Visualization**:\n",
    "   - Prediction vs true value plots for train/val sets\n",
    "   - Embedding visualization with t-SNE/UMAP\n",
    "   - Comparison with traditional ML models\n",
    "   - Hyperparameter experimentation\n",
    "\n",
    "### Model Architecture Features:\n",
    "- Hierarchical graph pooling with attention\n",
    "- Structure learning capabilities\n",
    "- Multi-level graph representations\n",
    "- Global pooling aggregation\n",
    "\n",
    "### Usage:\n",
    "```python\n",
    "# Basic usage\n",
    "config = HGPSLConfig()\n",
    "model, results = train_hgpsl_model(config, max_epochs=100)\n",
    "\n",
    "# With custom parameters\n",
    "config.nhid = 256\n",
    "config.pooling_ratio = 0.3\n",
    "model, results = train_hgpsl_model(config, loss='mae', lr=1e-3)\n",
    "```\n",
    "\n",
    "This provides a comprehensive framework for testing and experimenting with HGPSL models\n",
    "on graph regression tasks using training and validation data.\n",
    "\"\"\")\n",
    "\n",
    "print(\"Testing completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
